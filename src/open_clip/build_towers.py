from .modified_resnet import ModifiedResNet
from .timm_model import TimmModel

from .configs import Vision_Embedding_Config, Perceiver_Config
from .vision_embedding_perceiver import VisionEmbedding

from dataclasses import dataclass

from typing import Any, Dict, Optional, Tuple, Union

from .transformer import LayerNormFp32, LayerNorm, QuickGELU, VisionTransformer
import torch
import torch.nn as nn
from functools import partial


@dataclass
class CLIPVisionCfg:
    layers: Union[Tuple[int, int, int, int], int] = 12
    width: int = 768
    head_width: int = 64
    mlp_ratio: float = 4.0
    patch_size: int = 16
    image_size: Union[Tuple[int, int], int] = 224

    ls_init_value: Optional[float] = None  # layer scale initial value
    patch_dropout: float = (
        0.0  # what fraction of patches to dropout during training (0 would mean disabled and no patches dropped) - 0.5 to 0.75 recommended in the paper for optimal results
    )
    attentional_pool: bool = (
        False  # whether to use attentional pooler in the last embedding layer (overrides pool_type)
    )
    attn_pooler_queries: int = 256  # n_queries for attentional pooler
    attn_pooler_heads: int = 8  # n heads for attentional_pooling
    no_ln_pre: bool = False  # disable pre transformer LayerNorm
    pos_embed_type: str = "learnable"
    final_ln_after_pool: bool = False  # apply final LayerNorm after pooling
    pool_type: str = "tok"
    output_tokens: bool = False
    act_kwargs: Optional[dict] = None
    norm_kwargs: Optional[dict] = None

    timm_model_name: Optional[str] = (
        None  # a valid model name overrides layers, width, patch_size
    )
    timm_model_pretrained: bool = (
        False  # use (imagenet) pretrained weights for named model
    )
    timm_pool: str = (
        "avg"  # feature pooling for timm model ('abs_attn', 'rot_attn', 'avg', '')
    )
    timm_proj: str = (
        "linear"  # linear projection for timm model output ('linear', 'mlp', '')
    )
    timm_proj_bias: bool = False  # enable bias final projection
    timm_drop: float = 0.0  # head dropout
    timm_drop_path: Optional[float] = None  # backbone stochastic depth


def _build_vision_tower(
    embed_dim: int,
    vision_cfg: CLIPVisionCfg,
    quick_gelu: bool = False,
    cast_dtype: Optional[torch.dtype] = None,
):

    if isinstance(vision_cfg, dict) and "perceiver_config" not in vision_cfg:
        vision_cfg = CLIPVisionCfg(**vision_cfg)
    else:
        vision_cfg = Vision_Embedding_Config(**vision_cfg)
        vision_cfg.embed_dim = embed_dim
        vision_cfg.quick_gelu = quick_gelu
        vision_cfg.cast_dtype = cast_dtype

    # OpenAI models are pretrained w/ QuickGELU but native nn.GELU is both faster and more
    # memory efficient in recent PyTorch releases (>= 1.10).
    # NOTE: timm models always use native GELU regardless of quick_gelu flag.
    act_layer = QuickGELU if quick_gelu else nn.GELU

    if isinstance(vision_cfg, CLIPVisionCfg):
        if vision_cfg.timm_model_name:
            visual = TimmModel(
                vision_cfg.timm_model_name,
                pretrained=vision_cfg.timm_model_pretrained,
                pool=vision_cfg.timm_pool,
                proj=vision_cfg.timm_proj,
                proj_bias=vision_cfg.timm_proj_bias,
                drop=vision_cfg.timm_drop,
                drop_path=vision_cfg.timm_drop_path,
                patch_drop=(
                    vision_cfg.patch_dropout if vision_cfg.patch_dropout > 0 else None
                ),
                embed_dim=embed_dim,
                image_size=vision_cfg.image_size,
            )
        elif isinstance(vision_cfg.layers, (tuple, list)):
            vision_heads = vision_cfg.width * 32 // vision_cfg.head_width
            visual = ModifiedResNet(
                layers=vision_cfg.layers,
                output_dim=embed_dim,
                heads=vision_heads,
                image_size=vision_cfg.image_size,
                width=vision_cfg.width,
            )
        else:
            vision_heads = vision_cfg.width // vision_cfg.head_width
            norm_layer = (
                LayerNormFp32
                if cast_dtype in (torch.float16, torch.bfloat16)
                else LayerNorm
            )
            if vision_cfg.norm_kwargs:
                norm_layer = partial(norm_layer, **vision_cfg.norm_kwargs)
            if vision_cfg.act_kwargs is not None:
                act_layer = partial(act_layer, **vision_cfg.act_kwargs)

            visual = VisionTransformer(
                image_size=vision_cfg.image_size,
                patch_size=vision_cfg.patch_size,
                width=vision_cfg.width,
                layers=vision_cfg.layers,
                heads=vision_heads,
                mlp_ratio=vision_cfg.mlp_ratio,
                ls_init_value=vision_cfg.ls_init_value,
                patch_dropout=vision_cfg.patch_dropout,
                attentional_pool=vision_cfg.attentional_pool,
                attn_pooler_queries=vision_cfg.attn_pooler_queries,
                attn_pooler_heads=vision_cfg.attn_pooler_heads,
                pos_embed_type=vision_cfg.pos_embed_type,
                no_ln_pre=vision_cfg.no_ln_pre,
                final_ln_after_pool=vision_cfg.final_ln_after_pool,
                pool_type=vision_cfg.pool_type,
                output_tokens=vision_cfg.output_tokens,
                output_dim=embed_dim,
                act_layer=act_layer,
                norm_layer=norm_layer,
            )
    elif isinstance(vision_cfg, Vision_Embedding_Config):
        if hasattr(vision_cfg, "perceiver_config"):
            vision_model = _build_vision_tower(
                embed_dim, vision_cfg.vision_tower_config, quick_gelu, cast_dtype
            )
            if "ModifiedResNet" in str(type(vision_model)):
                vision_model.attnpool = nn.Identity()
            vision_cfg.vision_model = vision_model
            vision_cfg.perceiver_config = Perceiver_Config(
                **vision_cfg.perceiver_config
            )
            vision_cfg.vision_tower = str(type(vision_model))
            visual = VisionEmbedding(vision_cfg)
        else:
            raise NotImplementedError
    else:
        raise NotImplementedError

    return visual
